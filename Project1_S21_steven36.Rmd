---
title: 'Project 1: Predict the Housing Prices in Ames'
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    theme: readable
    toc: yes
    toc_float: yes
date: "Spring 2021"
---

## Ames Housing Data

### Data set

Download the dataset, `Ames_data.csv`, from the Resouces page on Piazza. The dataset has 2930 rows (i.e., houses) and 83 columns. 

* The first column is "PID", the Parcel identification number; 
* The last column is the response variable, `Sale_Price`;
* The remaining 81 columns are explanatory variables describing (almost) every aspect of residential homes.

### Test IDs
  
Download `project1_testIDs.dat` from the Resouces page on Piazza. This file contains 879 rows and 10 columns, which will be used to generate **10 sets** of training/test splits from `Ames_data.csv`. Each column contains the 879 row-numbers of a test data.  

Here is how you generate a split of training and test using the j-th column of `project1_testIDs.dat` in R. 

```{r eval = FALSE}
data <- read.csv("Ames_data.csv")
testIDs <- read.table("project1_testIDs.dat")
j <- 2
train <- data[-testIDs[,j], ]
test <- data[testIDs[,j], ]
test.y <- test[, c(1, 83)]
test <- test[, -83]
write.csv(train,"train.csv",row.names=FALSE)
write.csv(test, "test.csv",row.names=FALSE)
write.csv(test.y,"test_y.csv",row.names=FALSE)
```
Save the training and test as csv files. In particular, the test data are saved as two seperate two files: one containing just the feature vectors and the other one containing the response column. 

For your remaining analysis, you need to read the training and test data from the csv files.

```{r eval = FALSE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

```{r}
head(data)
```

```{r}
names(data)
```


```{r}
no_cols = dim(data)[2]
counts_col = rep(-1, no_cols)
for (i in 1:83){
  s = sum(as.integer(is.na(data[,i])))
  counts_col[i] = s 
}
no_NAs = sum(counts_col)
empty_i = which(counts_col>1)
names(data[empty_i])  #find columns which has NA values

```


### Goal
The goal is to predict the final price of a home (**in log scale**) with those explanatory variables. You need to build **Two** prediction models selected from the following three: 


* one based on linear regression models with Lasso or Ridge or Elasticnet penalty;
* one based on tree models, such as randomForest or boosting tree;


The features for the two models do not need to be the same. Please check Piazza for packages students are allowed to use for this project. 

### Source

* De Cock, D. (2011). "Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project," Journal of Statistics Education, Volume 19, Number 3. [[PDF](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf)]

* Check variable description [[Here](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt)]

* This data set has been used in a Kaggle competition (https://www.kaggle.com/c/house-prices-advanced-regression-techniques). You can check how others analyze this data and try some sample code on Kaggle. Note that our data set has two more explanatory variables, "Longitude" and "Latitude."

## What to Submit

Submit the following **two** items on Coursera:

* <font color="red">R/Python code</font> in a single file named **mymain.R** (or **mymain.py**) that takes a training data and a test data as input and outputs two submission files (the format of the submission file is given below). **No zip file; no R markdown file**.

The structure of your mymain.R (or mymain.py) should look like the following. **Note** that you have to pre-process your training and test data separately. 

```{r}
# Step 0: Load necessary libraries
###########################################
# Step 0: Load necessary libraries
#
# YOUR CODE
# 
library(glmnet)

###########################################
# Step 1: Preprocess training data
#         and fit two models
#
train <- read.csv("train.csv")
#
# YOUR CODE
# 


```

```{r}

#replace NAs with 0 in predictor Garage_Yr_built
#input data is a dataframe w/ X predictors and output variable, y 
Reg_preprocess_traindata = function(traindata){
  for (i in 1 : length(traindata$Garage_Yr_Blt)){
    if (is.na(traindata$Garage_Yr_Blt[i]) == TRUE){
      traindata$Garage_Yr_Blt[i] = 0
    }
  }
  inp_matrix = model.matrix(PID ~ ., traindata)  #convert to matrix with dummy variables in place of cat, ignore PID it will not be included in the inp_matrix
  inp_matrix = inp_matrix[, -1]  #remove intercept col
  return (inp_matrix)
}
```



```{r}
set.seed(0303)
inp_matrix = Reg_preprocess_traindata(traindata = train)
X_matrix = inp_matrix[ , 1:dim(inp_matrix)[2] - 1]
y = inp_matrix[ , dim(inp_matrix)[2]]

myRidge = glmnet(X_matrix, log(y), alpha = 0) 
myLasso = glmnet(X_matrix, log(y), alpha = 1)

```

```{r}

ridgeCV = cv.glmnet(X_matrix, log(y), alpha = 0, lambda = exp(seq(-6, 1, length = 80)))
plot(ridgeCV)

```

```{r}

lassoCV = cv.glmnet(X_matrix, log(y), alpha = 1, lambda = exp(seq(-8, -2, length = 80)))
plot(lassoCV)

```

```{r}
ridge_minLamba = ridgeCV$lambda.min
ridge_1seLamba = ridgeCV$lambda.1se

lasso_minLamba = lassoCV$lambda.min
lasso_1seLamba = lassoCV$lambda.1se

```


Test data preprocessing tasks:
To perform prediction using our statistical model requires that the train and test data have the same predictors.  In our case, the test data may not have the same predictors after assigining dummy variables to a categorical/ordinal variable.  This is because the the test data may not include all the levels seen the train data or vise versa.  For example, let's suppose that the data set has a variable called "size" which has 5 levels, XS, S, M, L, XL.  The train data includes four of the five levels XS, S, M, L and the test data has three of the five levels XS, S, XL. The test data must match the train data predictors and since the train data doesn't include XL we exclude this level in the test data.  In the converse situation where the train data includes the L level but the test data does not, we make a new L dummy variable composed of 0s in the test data.  Where both train and test data share the same dummy variables we leave as is.               

The glmnet() requires numerical data in a matrix form therefore factor variables cannot be used.

```{r}

###########################################
# Step 2: Preprocess test data
#         and output predictions into two files
#
test <- read.csv("test.csv")
#
# YOUR CODE
# 


```

```{r}
Reg_preprocess_testdata = function(testdata, X_matrix){
  
#need to preprocess test_matarix data so that all x predictors are identical after dummy #variables were created for categorical variables; need to modify the test_matrix
#replace NAs with 0 in predictor Garage_Yr_built
  for (i in 1 : length(testdata$Garage_Yr_Blt)){
    if (is.na(testdata$Garage_Yr_Blt[i]) == TRUE){
      testdata$Garage_Yr_Blt[i] = 0
    }
  }
  #testdata = na.omit(testdata)
  test_matrix = model.matrix(PID ~ ., testdata)  #PID has no significance here and is not included in matrix
  test_matrix = test_matrix[ ,-1]  #remove the intercept column
  
  train_names = names(as.data.frame(X_matrix))
  test_names = names(as.data.frame(test_matrix))
  
  #create a new df to store the updated test_matrix w same number predictors as X_matrix
  n_columns = ncol(X_matrix)
  n_rows = nrow(test_matrix)
  test_df0 = as.data.frame(matrix(rep(0, n_columns * n_rows), ncol = ncol(X_matrix), nrow = nrow(test_matrix)))
  names(test_df0) = train_names
  
  for (i in 1:length(train_names)){
    if (sum(test_names==train_names[i]) == 1){  #if train_name found in test_names insert data into df0
      test_df0[train_names[i]] = test_matrix[, train_names[i]]
    }
  }
  return (as.matrix(test_df0))
}
```


```{r}
test_matrix = Reg_preprocess_testdata(testdata = test, X_matrix = X_matrix)

ridge_predict = exp(predict(myRidge, s = ridge_1seLamba, newx = test_matrix))[,1]  #price in real dollars 
lasso_predict = exp(predict(myLasso, s = lasso_minLamba, newx = test_matrix))[,1]

```

```{r}
class(ridge_predict)
```


```{r}
RMSE_ridge = sqrt(mean((log(ridge_predict) - log(test.y$Sale_Price))^2))
RMSE_lasso = sqrt(mean((log(lasso_predict) - log(test.y$Sale_Price))^2))
```

```{r eval = FALSE}
#run all 10 splits for both ridge and lasso models
library(glmnet)
set.seed(0303)
data <- read.csv("Ames_data.csv")
testIDs <- read.table("project1_testIDs.dat")
summary_table = data.frame("lasso_rmse_minLambda" = rep(0, 10), "lasso_rmse_1seLambda" = rep(0, 10), "ridge_rmse_minLambda" = rep(0, 10), "ridge_rmse_1seLambda" = rep(0, 10) )
for (j in 1:10){
  train <- data[-testIDs[,j], ]
  test <- data[testIDs[,j], ]
  test.y <- test[, c(1, 83)]
  test <- test[, -83]
  write.csv(train,"train.csv",row.names=FALSE)
  write.csv(test, "test.csv",row.names=FALSE)
  write.csv(test.y,"test_y.csv",row.names=FALSE)
  
  train <- read.csv("train.csv")
  inp_matrix = Reg_preprocess_traindata(traindata = train)
  X_matrix = inp_matrix[ , 1:dim(inp_matrix)[2] - 1]
  y = inp_matrix[ , dim(inp_matrix)[2]]

  myRidge = glmnet(X_matrix, log(y), alpha = 0) 
  myLasso = glmnet(X_matrix, log(y), alpha = 1)
  
  ridgeCV = cv.glmnet(X_matrix, log(y), alpha = 0, lambda = exp(seq(-6, 1, length = 80)))
  #plot(ridgeCV)
  lassoCV = cv.glmnet(X_matrix, log(y), alpha = 1, lambda = exp(seq(-8, -2, length = 80)))
  #plot(lassoCV)
  
  ridge_minLamba = ridgeCV$lambda.min
  ridge_1seLamba = ridgeCV$lambda.1se

  lasso_minLamba = lassoCV$lambda.min
  lasso_1seLamba = lassoCV$lambda.1se
  
  test <- read.csv("test.csv")
  test_matrix = Reg_preprocess_testdata(testdata = test, X_matrix = X_matrix)
  
  ridge_predict_min = exp(predict(myRidge, s = ridge_minLamba, newx = test_matrix))[,1]  #price in real dollars
  ridge_predict_1se = exp(predict(myRidge, s = ridge_1seLamba, newx = test_matrix))[,1]
  
  lasso_predict_min = exp(predict(myLasso, s = lasso_minLamba, newx = test_matrix))[,1]
  lasso_predict_1se = exp(predict(myLasso, s = lasso_1seLamba, newx = test_matrix))[,1]
  
  RMSE_ridge_min = sqrt(mean((log(ridge_predict_min) - log(test.y$Sale_Price))^2))
  RMSE_ridge_1se = sqrt(mean((log(ridge_predict_1se) - log(test.y$Sale_Price))^2))
  RMSE_lasso_min = sqrt(mean((log(lasso_predict_min) - log(test.y$Sale_Price))^2))
  RMSE_lasso_1se = sqrt(mean((log(lasso_predict_1se) - log(test.y$Sale_Price))^2))
  
  summary_table[j, 1] = RMSE_lasso_min
  summary_table[j, 2] = RMSE_lasso_1se
  summary_table[j, 3] = RMSE_ridge_min
  summary_table[j, 4] = RMSE_ridge_1se
}
```

```{r}
summary_table
```

```{r}
colMeans(summary_table)
```

* <font color="red">A report</font> (3 pages maximum, PDF or HTML) that provides 

  1. technical details ( e.g., pre-processing, implementation details if not trivial) for the models you use, and

  2. any interesting findings from your analysis.

  3. In addition, report the accuracy on the test data (see evaluation metric given below), running time of your code and the computer system you use (e.g., Macbook Pro, 2.53 GHz, 4GB memory, or AWS t2.large) for the 10 training/test splits. You **do not** need to submit the part of the code that evaluates test accuracy.

  In your report, describe the techincal details of your code, summarize your findings, and **do not copy-and-paste your code to the report**. 


## Code Evaluation

We will run the command "source(mymain.R)" in a directory, in which there are only **two files: train.csv and test.csv**, one of the 10 training/test splits.

* train.csv:  83 columns;
* test.csv: 82 columns without the column "Sale_Price".

After running your code, we should see **Two** txt files in the same directory named "mysubmission1.txt" and "mysubmission2.txt." Each submission file contains prediction on the test data from a model.

**Submission File Format**. The file should have the following format (do not forget the **comma** between PID and Sale_Price):

<pre>
PID,  Sale_Price 
528221060,  169000.7 
535152150,  14523.6 
533130020,  195608.2 
</pre>

**Evaluation Metric**. Submission are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted price and the logarithm of the observed sales price. Our evaluation R code looks like the following:

<pre>
# Suppose we have already read test_y.csv in as a two-column 
# data frame named "test.y":
# col 1: PID
# col 2: Sale_Price
```{r}
pred <- read.csv("lasso.txt")
names(test.y)[2] <- "True_Sale_Price"
pred <- merge(pred, test.y, by="PID")
sqrt(mean((log(pred$Sale_Price) - log(pred$True_Sale_Price))^2))
```
</pre>

**Performance Target**. Your performance is based on the **minimal** RMSE from the two models. Full credit for submissions with minimal RMSE less than

* <font color="red">**0.125**</font> for the first 5 training/test splits and
* <font color="red">**0.135**</font> for the remaining 5 training/test splits.

## FAQ

*  *Can I find the 10 training/test datasets on Piazza?*

    You generate them from `project1_testIDs.dat` and `Ames_data.csv`

* *Should we download the training/test datasets from Kaggle and upload our prediction on Kaggle for evaluation?*

    No. You do not need to download/upload any dataset from/to Kaggle.
    
* *Should we include the split data part in the function we write?*

  No, you do not need to include the split data part in your code. Your code should start with loading the train.csv. Build your models based on train.csv, and then load test.csv and output the corresponding prediction. Store your output in txt files following the Submission File Format described above.

* *Do we need to do diagnostics? Is it important for this data set? If it is, should we delete any extreme values?*

  Diagnostics or any pre-processing for missing values and extreme values are done by you. Please include your pre-processing procedure in the code and its description in the report.
  
  Please keep in mind that you are **NOT** asked to report detailed EDA (exploratory data analysis), but to build predictive models based on this data set.
  
* *The RMSE of the logarithm of price is infinity/nan for some observations. What should I do?*

  Recall that log(y) is only defined for y > 0. If you train a model to predict y without any constraints it is possible that your model may predict a zero or negative value. These zero or negative predictions will cause infinities and nans to pop up in the metric calculations.

  Since what matters in the evaluation metric is log(y), predict log(y) or in this case log(Sales_Price) instead of the untransformed target. In other words,  build a model to predict the logarithm of the price. 
  
  Since you are asked to output the predicted Sales_Price in  "mysubmission1.txt" and "mysubmission2.txt," you need to transform the target back using the exponential -- exp(pred) -- when submitting your results.  
  
* *How to specify tuning parameters?*

  For Lasso or any algorithm that has just one or two tuning parameters, suggest to use their built-in CV procedures to select tuning parameters.
  
  Some algorithms such as boosting treess have quite a few tuning parameters. It would be time-consuming to tune them on a grid for each new training. For this assignment, it is okay to tune them based on the 10 splits of training/test, and then in your submitted code, fix the value of some (or even all) tuning parameter to save computation time. 
  
