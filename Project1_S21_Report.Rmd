---
title: "Project 1: Predict the Housing Prices in Ames"
date: "Spring 2021"
output:
  html_document:
    theme: readable
    toc: TRUE
    toc_float: TRUE
---

# Team Members and Contributions (all MCS)
Steve Su (steven36)


Pierson Wodarz (wodarz2)


Steve and Pierson both built several models which met the performance criteria, and chose two models out of these options. Both reviewed the work of each other for understanding, corrections, and potential improvements. Feedback was provided to each other over calls, while updates to the parts were made by the respective team members. The report work was broken out by model, with each taking a model to add details. 

# Introduction
## Data
The dataset contains data on 2,930 houses in Ames, Iowa compiled by Dean De Cock. There are 83 datapoints for each house, including the sale price of the house and other information about the house such as its square footage and geolocation. 

## Goal
Using the datapoints available for each house, we look to build two models to accurately predict the sale price of a given house. We built the following models which generate our predictions:

1. Gradient boosting decision tree model (GBM)
2. A linear regression model with an ElasticNet penalty (ElasticNet)

# Data Preprocessing 
## GBM
**Train data preprocessing:**

Data preprocessing was negligible for the boosting tree model. The only preprocessing for training was to transform character fields to factor.

**Test data preprocessing: **

For the test data for the boosting tree model, we only needed to ensure that the character fields were transformed to factors with the same levels/values as those in the train dataset. Therefore, the character fields were transformed to factors using the levels from the train dataset.

## ElasticNet
**Train data preprocessing:**

Remove variables: Some predictors were removed because they were not beneficial to the model. There are several reasons this could be the case, such as variables with a high frequency in only one category, or variables which don't have a clear relationship to price at all. For example, the character variable of street would result in many distinct categories, and a disparity in streets between train and test would result in many NAs when factorizing, making the variable meaningless. Additionally, this information may be captured by other neighborhood categories. 

One Hot Encoding: For better results, we perform one-hot encoding using dummy variables trained on the character/categorical fields in the test data. Factor/character variables where converted to k-1 dummy variables where k corresponds to the levels of the categorical variable. Those dummy variables are then used to create additional one hot encoding columns, with the values representing whether a particular property had the categorical value for that category. We then removed the factorized categorical columns as the data is contained within the one hot encoding columns. 

Replace NA: We found that NANs existed within the "yr_garage_built" variable. We replaced NANs with 0, as our model cannot process NAN and replacing NAN with 0 allows the model to process these properties without significantly negatively impacting performance. 

Winsorization: Certain numerical values usually only have a linear influence on price within a specific range, above which the value of the house is not as significantly impacted by a corresponding increase in the value of the predictor. As a result, we windsorize the data to the 95th percentile for several of these numerical fields. 

**Test data preprocessing:**

We performed the same preprocessing for our test data as we did for our training data: removing variables, one hot encoding, replacing NA values, and winsorization. The only difference was in the preprocessing of the character/categorical values. Similar to the test data preprocessing for GBM, we needed to ensure that the character fields were transformed into one hot encodings with encodings equivalent to those from the train data set. In this case, we encode into dummy variables using the predict function where the model is created by the `dummyVars` function from the `caret` package and the data for this model is the train data. This ensures that the results are encodings from the train dataset or NA (for values in the test dataset which weren't present in the train dataset). For the NA values, they were simply set to 0. This is appropriate for one-hot encoding as a value that is outside the existing range will be encoded into 0 within the range.  

# Model Technical Details
## GBM
**Model description:** We used the `gbm` package in r to fit a gradient boosting model. This model uses a group of decision trees, each of which is a weak or shallow decision tree model. By improving the decision trees successively, the final model has powerful predictive performance.

**Tuning parameters:** The tuning parameters were set statically for all train and test splits/subsets and do not change between the splits/subsets. Instead, the parameters were tuned on the overall dataset, and reusing the same tuning parameters produced good performance on any observed split/subset of the data. We tuned the parameters manually, using the tuning parameters from https://uc-r.github.io/gbm_regression as a starting point. 

distribution = gaussian: A gaussian distribution is used for regression as is the case here. 

n.trees = 500: 500 trees produced the best performance across the various train/test splits when manually tuning parameters using steps of ~50 trees. 

shrinkage = 0.1: A shrinkage of .1 produced optimal results. 

interaction.depth = 5: Above 5 or below 5 reduced the overall performance. This is because below 5 was too shallow but above 5 was too deep. 

bag.fraction = 1: We train on the entirety of the training set to produce the best performance. 

cv.folds = 5: 5 fold cross validation was a good balance between training time and performance, with performance not significantly improving with additional cross validation (e.g. 10 fold).

## ElasticNet
**Model description:**

**Tuning parameters:**


# Results

## Accuracy 
| Test Split | GBM RMSE Test Error | ElasticNet RMSE Test Error |
|------------|---------------------|----------------------------|
| 1          | 0.1163156           | 0.1226382                  |
| 2          | 0.1160827           | 0.1179175                  |
| 3          | 0.1088356           | 0.120597                   |
| 4          | 0.1132515           | 0.1198059                  |
| 5          | 0.1083723           | 0.1114046                  |
| 6          | 0.1230553           | 0.1336597                  |
| 7          | 0.1266367           | 0.1270781                  |
| 8          | 0.1168903           | 0.1208161                  |
| 9          | 0.1261995           | 0.1299768                  |
| 10         | 0.1193281           | 0.123419                   |

## Running time
Running time = 3.348801 minutes

System specs: 

  * Processor:	Intel(R) Core(TM) i5-1035G1 CPU @ 1.00GHz, 1190 Mhz, 4 Core(s), 8 Logical Processor(s)
  * RAM: 8.00 GB

# Findings
Before finding a successful regression model using elasticnet, we ran both ridge and lasso penalty regression models.  The ridge and lasso performed similarly and was just shy of hitting the performance benchmarks.  In both models, the minimum lambda was a better choice than the one standard error lambda value.  On average (10 splits) the minimum lambda showed between 5-7% improvement in RMSE value compared to the one standard error lambda for both the lasso and ridge models.  With this information we also used the minimum lambda in our elasticnet model.  Before giving up on the lasso model we also refit the model using lm() function with the non-zero beta coefficients of the one standard error lambda model.  We saw an average improvement (10 splits) of 6% in RMSE reduction.  In the minimun lambda lasso model we did not see any improvement with refit but rather a slight degredation in performance.  In our final model, we used the elasticnet penalty in order to meet the performance benchmarks.  An alpha level of 0.2 was used.  An alpha level closer to zero behaves more closely to a ridge penalty.    

* GBM required significantly less preprocessing, but was more sensitive to tuning parameters

# Resources
https://uc-r.github.io/gbm_regression





