---
title: "Project 1: Predict the Housing Prices in Ames"
date: "Spring 2021"
output:
  html_document:
    theme: readable
    toc: TRUE
    toc_float: TRUE
---

# Team Members and Contributions (all MCS)
Steve Su (steven36)


Pierson Wodarz (wodarz2)


Steve and Pierson both built several models which met the performance criteria, and chose two models out of these options. Both reviewed the work of each other for understanding, corrections, and potential improvements. Feedback was provided to each other over calls, while updates to the parts were made by the respective team members. The report work was broken out by model, with each taking a model to add details. 

# Introduction
## Data
## Goal

# Data Preprocessing 
## GBM
Train data preprocessing: 
Data preprocessing was negligible for the boosting tree model. The only preprocessing for training was to transform character fields to factor.

Test data preprocessing: 
For the test data for the boosting tree model, we only needed to ensure that the character fields were transformed to factors with the same levels/values as those in the train dataset. Therefore, the character fields were transformed to factors using the levels from the train dataset.

## Elastic Net
Train data preprocessing:
Unlike the GBM model which uses factor variables, glmnet() requires input data to be in a matrix consisting of numerical data.  Therefore factor variables where converted to k-1 dummy variables where k corresponds to the levels of the categorical variable.  Secondly, NaNs were replaced with zeros.  NaNs were only found in the predictor, "yr_garage_built", which corresonded to observations with no garage. Thirdly, some predictors were removed because they were not beneficial to the model.  For example, "longitude" and "latitude" by themselves don't have much meaning.  It only makes sense when these two variables are used together to point to a location.  Lastly, numerical data was winsorized to give less influence to extremely high values, above the 95th quartile.  

Test data preprocessing:
Similar to the test data preprocessing for GBM, we needed to ensure that the character fields were transformed into one hot encodings with encodings equivalent to those from the train data set. In this case, we encode into dummy variables using the predict function where the model is created by the 'dummyVars' function from the 'caret' package and the data for this model is the train data. This ensures that the results are encodings from the train dataset or NA (for values in the test dataset which weren't present in the train dataset). For the NA values, they were simply set to 0. This is appropriate for one-hot encoding as a value that is outside the existing range will be encoded into 0 within the range.  
As with training data, we filled all NAs in "yr_garage_built" with zeros. Lastly, we also applied winsoring of numerical data and removed extraneous predictors as in the case with the training data. 




# Model Technical Details
## Model description
## Tuning parameters

# Results

## Accuracy 
| Test Split | GBM Test Error | ElasticNet Test Error |
|------------|----------------|-----------------------|
| 1          | 0.1163156      | 0.1226382             |
| 2          | 0.1160827      | 0.1179175             |
| 3          | 0.1088356      | 0.120597              |
| 4          | 0.1132515      | 0.1198059             |
| 5          | 0.1083723      | 0.1114046             |
| 6          | 0.1230553      | 0.1336597             |
| 7          | 0.1266367      | 0.1270781             |
| 8          | 0.1168903      | 0.1208161             |
| 9          | 0.1261995      | 0.1299768             |
| 10         | 0.1193281      | 0.123419              |

## Running time
Running time = 3.348801 minutes

System specs: 

  * Processor:	Intel(R) Core(TM) i5-1035G1 CPU @ 1.00GHz, 1190 Mhz, 4 Core(s), 8 Logical Processor(s)
  * RAM: 8.00 GB

# Findings
Before finding a successful regression model using elasticnet, we ran both ridge and lasso penalty regression models.  The ridge and lasso performed similarly and was just shy of hitting the performance benchmarks.  In both models, the minimum lambda was a better choice than the one standard error lambda value.  On average (10 splits) the minimum lambda showed between 5-7% improvement in RMSE value compared to the one standard error lambda for both the lasso and ridge models.  With this information we also used the minimum lambda in our elasticnet model.  Before giving up on the lasso model we also refit the model using lm() function with the non-zero beta coefficients of the one standard error lambda model.  We saw an average improvement (10 splits) of 6% in RMSE reduction.  In the minimun lambda lasso model we did not see any improvement with refit but rather a slight degredation in performance.  In our final model, we used the elasticnet penalty in order to meet the performance benchmarks.  An alpha level of 0.2 was used.  An alpha level closer to zero behaves more closely to a ridge penalty.    

* GBM required significantly less preprocessing, but was more sensitive to tuning parameters





